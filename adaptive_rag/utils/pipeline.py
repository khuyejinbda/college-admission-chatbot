from adaptive_rag.utils import tools, safeguard, search, generate, memory, mongoDB, router, slang, state
from adaptive_rag.utils.state import AdaptiveRagState

from typing import TypedDict, List
from langchain_core.documents import Document
from langgraph.graph import StateGraph, START, END
from IPython.display import Image, display
from functools import partial

import random
import threading
import sys
from typing import Dict, Any, Union # Added Union

# íˆ´ ì„¤ì • í•¨ìˆ˜
def set_tools():
    """
    Set up the tools for the Adaptive RAG pipeline.
    """
    tools_list = [
        tools.search_policy,
        tools.search_admission,
        tools.search_book,
        tools.search_subject,
        tools.search_seteuk
    ]
    
    return tools_list

# íˆ´ ì„¤ì •
tools = set_tools()

def build_adaptive_rag() -> StateGraph:
    """
    LangGraph ê¸°ë°˜ Adaptive RAG ì±—ë´‡ì„ ìœ„í•œ ìƒíƒœ ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜.
    ê° ë…¸ë“œë“¤ì€ ì§ˆë¬¸ ì²˜ë¦¬ì˜ ë‹¨ê³„(ìš•ì„¤ í•„í„°ë§ â†’ ë¼ìš°íŒ… â†’ ê²€ìƒ‰ â†’ ê´€ë ¨ì„± í‰ê°€ â†’ ìƒì„±)ë¡œ êµ¬ì„±ë¨.

    Returns:
        StateGraph: ì‹¤í–‰ ê°€ëŠ¥í•œ ìƒíƒœ ê·¸ë˜í”„

    - êµ¬ì¡°
    [profanity_prevention] 
    â†’ (clean) â†’ [route_question_adaptive] 
        â†’ [search_xxx] 
            â†’ [check_relevance]
                â†’ (1) [generate] â†’ end
                â†’ (0, not retried) â†’ [re_route_question_adaptive] â†’ [search_xxx2] â†’ ...
                â†’ (0, retried) â†’ [llm_fallback] â†’ end
    â†’ (profane) â†’ end

    """

    # ê·¸ë˜í”„ ì´ˆê¸°í™” (state íƒ€ì…ì€ AdaptiveRagState)
    builder = StateGraph(AdaptiveRagState)

    # ì‹œì‘ ì§€ì ì„ ìš•ì„¤ í•„í„°ë§ ë…¸ë“œë¡œ ì§€ì •
    builder.set_entry_point("profanity_prevention")

    # === ì£¼ìš” ë…¸ë“œ ë“±ë¡ ===

    # 1. ìš•ì„¤ í•„í„°ë§ (ìš•ì„¤ ê°ì§€ ë° ì¢…ë£Œ/ê³„ì† íŒë‹¨)
    builder.add_node("profanity_prevention", partial(safeguard.profanity_prevention))

    # 2. ë¼ìš°íŒ… (ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ search ë…¸ë“œ ê²°ì •)
    builder.add_node("route_question_adaptive", router.route_question_adaptive)
    builder.add_node("re_route_question_adaptive", router.re_route_question_adaptive)

    # 3. ê´€ë ¨ì„± íŒë‹¨ (ê²€ìƒ‰ ê²°ê³¼ì™€ ì§ˆë¬¸ì´ ì—°ê²°ë˜ëŠ”ì§€ íŒë‹¨)
    builder.add_node("check_relevance", check.check_relevance)

    # 4. ê²€ìƒ‰ ë…¸ë“œ (ì£¼ì œë³„ë¡œ ë¶„ë¦¬)
    builder.add_node("search_policy", search.search_policy_adaptive)
    builder.add_node("search_subject", search.search_subject_adaptive)
    builder.add_node("search_admission", search.search_admission_adaptive)
    builder.add_node("search_book", search.search_book_adaptive)
    builder.add_node("search_seteuk", search.search_seteuk_adaptive)

    # 5. ì‘ë‹µ ìƒì„± ë˜ëŠ” fallback
    builder.add_node("generate", generate.generate_adaptive)
    builder.add_node("llm_fallback", generate.llm_fallback_adaptive)

    # === ìƒíƒœ ê°„ ì—°ê²° ì •ì˜ ===

    # Step 1: ìš•ì„¤ì´ ê°ì§€ë˜ë©´ ì¢…ë£Œ, ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
    builder.add_conditional_edges(
        "profanity_prevention",
        safeguard.check_profanity_result,
        {
            "__end__": "__end__",  # ìš•ì„¤ ê°ì§€ ì‹œ ì¢…ë£Œ
            "route_question_adaptive": "route_question_adaptive",  # ì •ìƒ ì§ˆë¬¸ â†’ ë¼ìš°íŒ…
        }
    )

    # Step 2: ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ê²€ìƒ‰ ë…¸ë“œë¡œ ì´ë™
    builder.add_conditional_edges(
        "route_question_adaptive",
        lambda state: state["next_node"],
        {
            "search_policy": "search_policy",
            "search_subject": "search_subject",
            "search_admission": "search_admission",
            "search_book": "search_book",
            "search_seteuk": "search_seteuk",
            "llm_fallback": "llm_fallback",
        }
    )

    # Step 3: ëª¨ë“  search ë…¸ë“œì˜ ê²°ê³¼ëŠ” check_relevanceë¡œ ì´ë™
    for node in ["search_policy", "search_subject", "search_admission", "search_book", "search_seteuk"]:
        builder.add_edge(node, "check_relevance")

    # Step 4: ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ generateë¡œ, ë‚®ê³  ì¬ì‹œë„ ê°€ëŠ¥í•˜ë©´ ì¬ë¼ìš°íŒ…, ì•„ë‹ˆë©´ fallback
    builder.add_conditional_edges(
        "check_relevance",
        lambda state: (
            "generate" if state.get("relevance_score") == 1
            else (
                "re_route_question_adaptive" if not state.get("retried", False)
                else "llm_fallback"
            )
        ),
        {
            "generate": "generate",
            "re_route_question_adaptive": "re_route_question_adaptive",
            "llm_fallback": "llm_fallback",
        }
    )

    # Step 5: ì¬ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ìƒˆë¡œìš´ ë…¸ë“œë¡œ ì´ë™
    builder.add_conditional_edges(
        "re_route_question_adaptive",
        lambda state: (
            "llm_fallback"  # ì¬ì‹œë„ì¸ë° ë˜ ê°™ì€ ë…¸ë“œë¼ë©´ fallback
            if state["next_node"] in state.get("visited_nodes", [])
            else state["next_node"]  # ìƒˆ ë…¸ë“œì´ë©´ í•´ë‹¹ ë…¸ë“œë¡œ
        ),
        {
            "search_policy": "search_policy",
            "search_subject": "search_subject",
            "search_admission": "search_admission",
            "search_book": "search_book",
            "search_seteuk": "search_seteuk",
            "llm_fallback": "llm_fallback",
        }
    )

    # Step 6: generate ë˜ëŠ” fallback ì´í›„ ì¢…ë£Œ
    builder.add_edge("generate", "__end__")
    builder.add_edge("llm_fallback", "__end__")

    # ê·¸ë˜í”„ ìµœì¢… ì»´íŒŒì¼
    return builder.compile()

# Global graph instance for API usage
compiled_graph_instance: Union[StateGraph, None] = None

def initialize_graph_for_api():
    """Initializes the RAG graph for API usage if not already initialized."""
    global compiled_graph_instance
    if compiled_graph_instance is None:
        print("Initializing RAG graph for API...")
        compiled_graph_instance = build_adaptive_rag()
        print("RAG graph initialized for API.")

def get_chatbot_response(question: str, user_id: str, category :str) -> Dict[str, Any]:
    """
    Processes a question using the RAG graph and returns the chatbot's response.
    Designed for API usage.
    """
    global compiled_graph_instance
    if compiled_graph_instance is None:
        # This should ideally be called by the FastAPI app startup,
        # but initialize here if accessed directly before app startup.
        initialize_graph_for_api()
    
    if compiled_graph_instance is None: # Still None after trying to initialize
        return {"error": "Graph could not be initialized."}

    inputs = {
        "question": question,
        "user_id": user_id,
        "category": category
    }
    final_node_output_state = {}
    
    # The stream yields dictionaries where keys are node names and values are the state dicts.
    # We want the state from the node that produces the 'generation'.
    for output_chunk in compiled_graph_instance.stream(inputs):
        for node_name, state_after_node in output_chunk.items():
            # We are interested in the state that contains the 'generation'.
            # This will typically be the state after 'generate' or 'llm_fallback' nodes.
            # The last such state before the stream ends should be the overall final state.
            final_node_output_state = state_after_node 

    if 'generation' not in final_node_output_state:
        # Check if it's an error or if the graph ended via a path that doesn't set 'generation'.
        # For now, we assume 'generate' or 'llm_fallback' always sets 'generation'.
        print(f"Warning: 'generation' key missing. Last state: {final_node_output_state}")
        return {"error": "Generation not found in the final state.", "details": final_node_output_state}

    return final_node_output_state

def run_chatbot():
    """
    ì±—ë´‡ ì‹¤í–‰ (5ë¶„ ë¬´ì‘ë‹µ ì‹œ ìë™ ì¢…ë£Œ)
    """
    graph = build_adaptive_rag()
    user_id = random.randint(1, 1_000_000)

    def timeout_exit():
        print("\n5ë¶„ ë™ì•ˆ ì…ë ¥ì´ ì—†ì–´ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(0)

    # ìµœì´ˆ íƒ€ì´ë¨¸ ì„¤ì •
    timer = threading.Timer(300, timeout_exit)
    timer.start()

    try:
        while True:
            question = input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” > ").strip()
            # ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ê¸°ì¡´ íƒ€ì´ë¨¸ ì·¨ì†Œ í›„ ì¬ì‹œì‘
            timer.cancel()
            if not question:
                print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            inputs = {
                "question": question,
                "user_id": user_id,
                "category": None  # ì¼ë‹¨ ë¡œì»¬ì—ì„œëŠ” ì—†ìŒ
            }

            for output in graph.stream(inputs):
                for key, value in output.items():
                    final_output = value

            print(f"ğŸ¤– ë‹µë³€: {final_output['generation']}")

            # ë‹¤ì‹œ 5ë¶„ íƒ€ì´ë¨¸ ì‹œì‘
            timer = threading.Timer(300, timeout_exit)
            timer.start()

    finally:
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ íƒ€ì´ë¨¸ëŠ” ë°˜ë“œì‹œ ì·¨ì†Œ
        timer.cancel()

# ì±—ë´‡ ì‹¤í–‰
if __name__ == "__main__":
    run_chatbot()
